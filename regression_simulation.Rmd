---
title: "Multivariable Regression Plot Simulations"
author: "Sven Halvorson"
date: "2020-04-10"
output:
  html_document:
    toc: yes
    toc_depth: 4
---


```{r setup, include=FALSE, warning = FALSE, message = FALSE}

# Libraries ---------------------------------------------------------------

### R Version & library path
options(knitr.kable.NA = '')

if(Sys.info()['login'] == 'halvors'){
  library('SvenR')
  .libPaths("C:/Users/HALVORS/Documents/rlibs")
}
# Libraries used
library("tidyverse")
library('summarytools')
library('snakecase')
library('lubridate')
library('here')
library('kableExtra')
library('simstudy')
library('fastDummies')
library('gridExtra')
library('grid')
```

As I mentioned on the [landing](https://svenhalvorson.github.io/party_page/portfolio/logistic_landing), the issue I sought to solve (or at least understand) is why my plots of logistic regression over aggregated data sometimes looked awful. Unfortunately, the [article](https://pdfs.semanticscholar.org/6807/8f0a3839b8d89cd25a964c03a571545793cb.pdf?_ga=2.241765888.362902951.1583340021-1552661845.1583340021) that my teammate gave me on marginal standardization didn't solve the main issue I was trying to fix. The difference between marginally standardized plots and plots with predictions at the mean were often similar and didn't address the weirdness I frequently saw.

I'm a considerably better programmer than statistician so I think I'll simulate a data set and do some experiments with different types of covariates. We'll get a chance to try out this `simstudy` library that I saw on a blog. Here's the [vignette](https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html) for reference. The code for these experiments can be founds [here](https://github.com/svenhalvorson/marg_std_and_simulations/blob/master/regression_simulation.Rmd).


#### Simulation Description

For this experiment I'm going to create two outcome variables. One will be a binary response with linear log odds changes through time. The incidence changes from the low 40s to high 50s through the entire time period. The other will be a normally distributed variable with a mean that moves from about -0.5 to 0.5 and a variance of 4. I've created roughly 1,000 observations for each of the 100 time points. Here are the aggregated values for each of the 100 times points:


```{r simdat, echo = FALSE, fig.align='center'}

# First set the time variable:
def_dat = defData(
  varname = 'time',
  dist = 'categorical',
  formula = rep(
    '0.01',
    times = 100
  ) %>%
    paste0(
      collapse = ';'
    )
) %>%
  # And the outcomes which will have some correlation with time:
  defData(
    varname = 'outcome_bin',
    dist = 'binary',
    formula = '0 + (time - 50)*0.004',
    link = 'logit'
) %>% 
  defData(
    varname = 'outcome_norm',
    dist = 'normal',
    formula = '(time - 50)*0.01',
    variance = 4
)

# Check a plot of these:
set.seed(0)
s1 = genData(
  100*1000,
  def_dat
) 
outcome_med_graph = s1 %>%
  select(time, outcome_bin) %>%
  group_by(time) %>%
  summarize(
    proportion = mean(outcome_bin)
  ) %>%
  ungroup() %>%
  ggplot(
    aes(x = time, y = proportion)
  ) +
  geom_point(color = 'orange', size = 2) +
  theme_minimal() +
  #facet_wrap(.~outcome) +
  labs(
    title = "Binary outcome's incidence by time",
    y = 'Incidence (%)',
    x = 'Time'
  ) +
  scale_y_continuous(
    limits = c(0, .85),
    labels = function(x){100*x}
  )

outcome_norm_graph = s1 %>%
  select(time, outcome_norm) %>%
  group_by(time) %>%
  summarize(
    proportion = mean(outcome_norm)
  ) %>%
  ungroup() %>%
  ggplot(
    aes(x = time, y = proportion)
  ) +
  geom_point(color = 'firebrick', size = 2) +
  theme_minimal() +
  #facet_wrap(.~outcome) +
  labs(
    title = "Normal variables's average by time",
    y = 'Mean',
    x = 'Time'
  ) +
  scale_y_continuous(
    limits = c(-2, 2)
  )

grid.arrange(outcome_med_graph, outcome_norm_graph, nrow = 1)

# Here we'll define the table of variables to be used:

iteration = 1:20
iteration_name = str_pad(
    string = iteration,
    width = 2,
    side = 'left',
    pad = '0'
)
incidence = c(-4.7, 0, 3.7, 0)
incidence_name = c('low', 'med', 'high', 'norm')
cor_type = c('uncor', 'time_cor', 'out_bin_cor', 'out_norm_cor', 'time_out_bin_cor', 'time_out_norm_cor')

predictor_table = tibble(
  iteration,
  iteration_name
) %>% 
  crossing(
    tibble(
      incidence,
      incidence_name
    )
  ) %>% 
  crossing(
    tibble(
      cor_type
    )
  )

predictor_table = predictor_table %>% 
  mutate(
    time_change = paste0(
      ' + (time-50)*0.01*',
      iteration,
      '/20'
    ),
    outcome_var = case_when(
      str_detect(cor_type, 'norm_') ~ 'outcome_norm',
      str_detect(cor_type, 'bin_') ~ 'outcome_bin',
      TRUE ~ ''
    ),
    out_change = paste0(
      ' + ',
      outcome_var,
      '*',
      iteration,
      '/20'
    ),
    forumla = case_when(
      cor_type == 'uncor' ~ as.character(incidence),
      cor_type == 'time_cor' ~ paste0(incidence, time_change),
      cor_type %in% c('out_bin_cor', 'out_norm_cor') ~ paste0(incidence, out_change),
      cor_type %in% c('time_out_bin_cor', 'time_out_norm_cor') ~ paste0(incidence, time_change, out_change)
    ),
    predictor_name = paste(
      cor_type,
      incidence_name,
      iteration_name,
      sep = '_'
    )
  ) %>% 
  select(
    predictor_name,
    incidence_name,
    forumla
  )

# Kinda wanna add a repetitive version of a bunch of these to see the effect of repeatedly
# adding other variables:
predictor_table_extra = predictor_table %>% 
  filter(
    grepl(
      'time_out_(norm|bin)_cor',
      x = predictor_name
    )
  ) %>% 
  crossing(
    tibble(
      extra = paste0(
        '_ex',
        str_pad(1:5, 2, 'left', '0')
      )
    )
  ) %>% 
  mutate(
    predictor_name = paste0(
      predictor_name,
      extra
    )
  ) %>% 
  select(-extra)

predictor_table = predictor_table %>% 
  bind_rows(predictor_table_extra)



for(i in 1:nrow(predictor_table)){
  def_dat = defData(
    def_dat,
    varname = predictor_table$predictor_name[i],
    dist = ifelse(
      predictor_table$incidence_name[i] == 'norm',
      'normal',
      'binary'
    ),
    formula = predictor_table$forumla[i],
    link = ifelse(
      predictor_table$incidence_name[i] == 'norm',
      'identity',
      'logit'
    ),
    variance = ifelse(
      predictor_table$incidence_name[i] == 'norm',
      4,
      0
    )
  )
}

set.seed(0)
sim_dat = genData(
  100*1000,
  def_dat
)

sim_dat = as_tibble(sim_dat)

```

After creating the outcome and time variables, I created a lot of binary and normal variables to act as covariates for our models. The binary versions were all created in sets of low, medium, and high incidence. For the predictors that were correlated with time and/or an outcome, they were created in sets of 20 where I gradually increased the strength of the relationships by 5% each time. The types were:

1. **Not correlated with either the time or outcome**
2. **Correlated with only the time variable**. 
3. **Correlated with only the outcome** 
4. **Correlated with both time and the predictor**.

Because I created the outcomes to be correlated with time, categories #2 & #3 are somewhat similar to the predictors in category #4. The difference is that the way I programmed this, the predictors that are listed as being only correlated with time or an outcome have a stronger, direct relationship.

A graph of the binary predictors that were of medium incidence and correlate with time only are depicted below:

```{r show_timecor, echo = FALSE, fig.align = 'center'}

sim_dat %>% 
  select(
    time,
    matches('time_cor_med')
  ) %>% 
  gather(
    key = 'iteration',
    value = 'val',
    -time
  ) %>% 
  group_by(
    time,
    iteration
  ) %>% 
  summarize(
    m = mean(val)
  ) %>% 
  ggplot() +
  geom_point(
    aes(x = time, y = m),
    color = 'peru'
  ) + 
  facet_wrap(.~iteration) +
  theme_minimal() +
  labs(
    y = 'Incidence (%)',
    title = 'Medium incidence predictor, correlated with time only'
  ) +
  scale_y_continuous(
    limits = c(0, 1),
    labels = function(x){100*x}
  )

```

```{r prediction_function, echo = FALSE, fig.align = 'center'}

# Now I would like to make functions that can take a model and predictor list:
# 1. Find the logistic line
# 2. Make a nice graph
# 3. Measure that average error from

get_slope_int = function(outcome, pred_list){
  # first make the model:
  form = paste0(
    outcome,
    ' ~ time + ',
    paste0(
      pred_list,
      collapse = ' + '
    )
  )
  family = ifelse(
    str_detect(outcome, 'norm'),
    'gaussian',
    'binomial'
  )

  mod = glm(
    form,
    family = family,
    data = sim_dat
  )

  # Then the predicted_values:
  pred_vals = sim_dat[, pred_list]
  pred_vals = pred_vals %>%
    group_by_all() %>%
    count() %>%
    ungroup() %>%
    mutate(
      rel_freq = n/sum(n),
      time = 0
    )
  pred_vals['pred_prob'] = predict.glm(
    mod,
    pred_vals,
    type = 'response'
  )
  pred_vals = pred_vals %>%
    summarize(
      intercept = sum(rel_freq*pred_prob)
    ) 
  if(family == 'binomial'){
    pred_vals = pred_vals %>%
    mutate(
      intercept = log(intercept/(1-intercept))
    ) 
  }
 
  # Now release the sloper intercept:
  c(
    slope = mod$coefficients['time'],
    intercept = pluck(pred_vals, 'intercept')
  )

}


```

#### Uncorrelated predictors

The first thing I want to do is look at some graphs of what happens when you use uncorrelated predictors (both normal and binary). We should probably think that this will not affect the shape of the graph very much because the effect on the outcome is constant through time. Here we'll just look at a graph of the outcomes vs time while we successively sample a larger number of predictors that are not correlated with either the outcome or time.

```{r uncorr_prd, echo = FALSE, fig.align = 'center'}

sample_slopes = function(n, outcome){

  predictors = predictor_table %>%
    filter(
      str_detect(
        predictor_name,
        'uncor'
      )
    ) %>%
    sample_n(n) %>%
    pluck('predictor_name')

  # call our other function:
  model_out = get_slope_int(outcome, predictors)
  tibble(
    outcome = outcome,
    n_predictors = n,
    slope = model_out[1],
    intercept = model_out[2]
  )

}

slope_table = lapply(
  (1:20)*2,
  sample_slopes,
  outcome = 'outcome_bin'
) %>%
  bind_rows()

slope_table= lapply(
  (1:20)*2,
  sample_slopes,
  outcome = 'outcome_norm'
) %>%
  bind_rows() %>%
  bind_rows(slope_table)

logit_rates = sim_dat %>%
  group_by(time) %>%
  summarize_at(
    vars(matches('outcome_')),
    mean
  ) %>%
  mutate(
    outcome_bin = log(outcome_bin/(1-outcome_bin))
  )

uncor_g1 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_bin
    )
  ) +
  geom_point(size = 2, color = 'orange') +
  geom_abline(
    data = filter(slope_table, outcome == 'outcome_bin'),
    aes(
      intercept = intercept,
      slope = slope,
      color = n_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Binary Outcome',
    y = 'Incidence (%)',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  scale_y_continuous(
    labels = function(x){
      round(100*exp(x)/(1+exp(x)),1)
    }
  ) +
  theme(
    legend.position = 'bottom'
  )

uncor_g2 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_norm
    )
  ) +
  geom_point(size = 2, color = 'firebrick') +
  geom_abline(
    data = filter(slope_table, outcome == 'outcome_norm'),
    aes(
      intercept = intercept,
      slope = slope,
      color = n_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Normal Outcome',
    y = 'Mean',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  theme(
    legend.position = 'bottom'
  )

grid.arrange(
  uncor_g1, 
  uncor_g2, 
  nrow = 1,
  top = textGrob("20 models with uncorrelated covariates")
)


```

All of the lines are basically on top of each other (the darkest shade is placed last) indicating that these graphs aren't really affected by uncorrelated predictors in the model. There also does not seem to be a cummulative effect of having more predictors.

#### Time correlated predictors

Here I'm going to do the same thing with predictors that are correlated with time only. We'll make 20 models, each time adding in a variable that has a slightly stronger correlation with time.


```{r time_corr_prd2, echo = FALSE, fig.align = 'center'}

sample_slopes2 = function(strength, outcome){

  strength_str = str_pad(strength, 2, 'left', '0')
  predictors = predictor_table %>%
    mutate(
      str = as.numeric(
        str_extract(
          predictor_name,
          '[0-9]{2}$'
        )
      )
    ) %>%
    filter(
      str <= strength,
      str_detect(
        predictor_name,
        'time_cor_'
      )
    ) %>%
    group_by(str) %>%
    sample_n(1) %>%
    pluck('predictor_name')

  # call our other function:
  model_out = get_slope_int(outcome, predictors)
  tibble(
    outcome = outcome,
    num_predictors = as.numeric(strength),
    slope = model_out[1],
    intercept = model_out[2]
  )

}

slope_table2 = lapply(
  (1:20),
  sample_slopes2,
  outcome = 'outcome_bin'
) %>%
  bind_rows()

slope_table2 = lapply(
  (1:20),
  sample_slopes2,
  outcome = 'outcome_norm'
) %>%
  bind_rows() %>% 
  bind_rows(slope_table2)


time_cor_g1 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_bin
    )
  ) +
  geom_point(size = 2, color = 'orange') +
  geom_abline(
    data = filter(slope_table2, outcome == 'outcome_bin'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Binary Outcome',
    y = 'Incidence (%)',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  scale_y_continuous(
    labels = function(x){
      round(100*exp(x)/(1+exp(x)),1)
    }
  ) +
  theme(
    legend.position = 'bottom'
  )


time_cor_g2 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_norm
    )
  ) +
  geom_point(size = 2, color = 'firebrick') +
  geom_abline(
    data = filter(slope_table2, outcome == 'outcome_norm'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Normal Outcome',
    y = 'Mean',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  theme(
    legend.position = 'bottom'
  )

grid.arrange(
  time_cor_g1, 
  time_cor_g2, 
  nrow = 1,
  top = textGrob("20 models with time correlated covariates")
)

```

If you look very closely you can see that the lines appears thinner at the center. This is due to the different regression lines being slightly different and not completely overlapping. The time correlated predictors do affect these graphs but in a minor way.

#### Outcome correlated predictors

What about predictors that are (directly) correlated with the outcome only? Here I'll sucessively add in a stronger predictor with each iteration:

```{r out_corr, echo = FALSE, fig.align = 'center'}

sample_slopes3 = function(strength, outcome){
  strength_str = str_pad(strength, 2, 'left', '0')
  predictors = predictor_table %>%
    mutate(
      str = as.numeric(
        str_extract(
          predictor_name,
          '[0-9]{2}$'
        )
      )
    ) %>%
    filter(
      str <= strength,
      str_detect(
        predictor_name,
        '^out_(bin|norm)_cor'
      ),
      !str_detect(
        predictor_name,
        'ex'
      )
    ) %>%
    group_by(str) %>%
    sample_n(1) %>%
    pluck('predictor_name')

  # call our other function:
  model_out = get_slope_int(outcome, predictors)
  tibble(
    outcome = outcome,
    predictors = paste(predictors, collapse = ', '),
    num_predictors = as.numeric(strength),
    slope = model_out[1],
    intercept = model_out[2]
  )

}

slope_table3 = lapply(
  (1:20),
  sample_slopes3,
  outcome = 'outcome_bin'
) %>%
  bind_rows()

slope_table3 = lapply(
  (1:20),
  sample_slopes3,
  outcome = 'outcome_norm'
) %>%
  bind_rows() %>% 
  bind_rows(slope_table3)


out_cor_g1 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_bin
    )
  ) +
  geom_point(size = 2, color = 'orange') +
  geom_abline(
    data = filter(slope_table3, outcome == 'outcome_bin'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Binary Outcome',
    y = 'Incidence (%)',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  scale_y_continuous(
    labels = function(x){
      round(100*exp(x)/(1+exp(x)),1)
    }
  ) +
  theme(
    legend.position = 'bottom'
  )


out_cor_g2 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_norm
    )
  ) +
  geom_point(size = 2, color = 'firebrick') +
  geom_abline(
    data = filter(slope_table3, outcome == 'outcome_norm'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Normal Outcome',
    y = 'Mean',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  theme(
    legend.position = 'bottom'
  )

grid.arrange(
  time_cor_g1, 
  time_cor_g2, 
  nrow = 1,
  top = textGrob("20 models with outcome correlated covariates")
)

```

Here we see basically the same thing, there is a little variation between the graphs but not a lot.

#### Time and outcome correlated predictors

Finally, we'll look at the predictors that are correlated with both time and the outcome. To start off, I'll create models with only time and one binary predictor of medium incidence, then successively increased the strength of the covariate. The unadjusted (time only) model is plotted in black.  We can see that the unadjusted model is closer to the models with weaker relationships to the outcome and time variables:

```{r time_out_cor_prd, echo = FALSE, fig.align = 'center'}

sample_slopes4 = function(strength, outcome){
  strength = str_pad(strength, 2, 'left', '0')
  short_name = ifelse(
    outcome == 'outcome_bin',
    'bin',
    'norm'
  )
  predictors = predictor_table %>%
    filter(
      str_detect(
        predictor_name,
        paste0('time_out_', short_name, '_cor_med_[0-9]')
      ),
      str_detect(
        predictor_name,
        strength
      ),
      !str_detect(
        predictor_name,
        'ex'
      )
    ) %>%
    sample_n(1) %>%
    pluck('predictor_name')

  # call our other function:
  model_out = get_slope_int(outcome, predictors)
  tibble(
    outcome = outcome,
    predictors = predictors,
    strength = 5*as.numeric(strength),
    slope = model_out[1],
    intercept = model_out[2]
  )

}

slope_table4 = lapply(
  (1:20),
  sample_slopes4,
  outcome = 'outcome_bin'
) %>%
  bind_rows()

slope_table4 = lapply(
  (1:20),
  sample_slopes4,
  outcome = 'outcome_norm'
) %>%
  bind_rows() %>% 
  bind_rows(slope_table4)

# make the unadjusted model too
unadjusted_bin = get_slope_int('outcome_bin', 'time')
unadjusted_norm = get_slope_int('outcome_norm', 'time')


time_out_cor_g1 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_bin
    )
  ) +
  geom_point(size = 2, color = 'orange') +
  geom_abline(
    data = filter(slope_table4, outcome == 'outcome_bin'),
    aes(
      intercept = intercept,
      slope = slope,
      color = strength
    ),
    size = 1.5
  ) +
  geom_abline(
    data = NULL,
    slope = unadjusted_bin[1],
    intercept = unadjusted_bin[2],
    color = 'black',
    linetype = 'dashed',
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Binary Outcome',
    y = 'Incidence (%)',
    x = 'Time',
    color = 'Strength'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  scale_y_continuous(
    labels = function(x){
      round(100*exp(x)/(1+exp(x)),1)
    }
  ) +
  theme(
    legend.position = 'bottom'
  )


time_out_cor_g2 = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_norm
    )
  ) +
  geom_point(size = 2, color = 'firebrick') +
  geom_abline(
    data = filter(slope_table4, outcome == 'outcome_norm'),
    aes(
      intercept = intercept,
      slope = slope,
      color = strength
    ),
    size = 1.5
  ) +
  geom_abline(
    data = NULL,
    slope = unadjusted_norm[1],
    intercept = unadjusted_norm[2],
    color = 'black',
    linetype = 'dashed',
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Normal Outcome',
    y = 'Mean',
    x = 'Time',
    color = 'Strength'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  theme(
    legend.position = 'bottom'
  )

grid.arrange(
  time_out_cor_g1, 
  time_out_cor_g2, 
  nrow = 1,
  top = textGrob("20 models with 1 time & outcome correlated predictor")
)


```

Thisis a very controlled example where we're just looking at the effect of one predictor and changing how strong its relationship is with both the outcome and time variable. In practice we're likely to have many  predictors of differing data types and incidences and strengths of relationships. Let's try an example where we successively add in an additional predictor of 5% stronger relationship. This time we'll randomly select from the various binary predictors and normal predictors at each level.

```{r time_out_corr_prd2, echo = FALSE, fig.align = 'center'}

sample_slopes5 = function(strength, outcome){
  strength_str = str_pad(strength, 2, 'left', '0')
  predictors = predictor_table %>%
    mutate(
      str = as.numeric(
        str_extract(
          predictor_name,
          '[0-9]{2}$'
        )
      )
    ) %>%
    filter(
      str <= strength,
      str_detect(
        predictor_name,
        '^time_out_'
      ),
      !str_detect(
        predictor_name,
        'ex'
      )
    ) %>%
    group_by(str) %>%
    sample_n(1) %>%
    pluck('predictor_name')

  # call our other function:
  model_out = get_slope_int(outcome, predictors)
  tibble(
    num_predictors = as.numeric(strength),
    predictors = paste(predictors, collapse = ', '),
    outcome = outcome,
    slope = model_out[1],
    intercept = model_out[2]
  )

}

slope_table5 = lapply(
  1:20,
  sample_slopes5,
  outcome = 'outcome_bin'
) %>%
  bind_rows()

slope_table5 = lapply(
  1:20,
  sample_slopes5,
  outcome = 'outcome_norm'
) %>%
  bind_rows() %>% 
  bind_rows(slope_table5)

time_out_cor_g1_multi = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_bin
    )
  ) +
  geom_point(size = 2, color = 'orange') +
  geom_abline(
    data = filter(slope_table5, outcome == 'outcome_bin'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  geom_abline(
    data = NULL,
    slope = unadjusted_bin[1],
    intercept = unadjusted_bin[2],
    color = 'black',
    linetype = 'dashed',
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Binary Outcome',
    y = 'Incidence (%)',
    x = 'Time',
    color = '# Predictors'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  scale_y_continuous(
    labels = function(x){
      round(100*exp(x)/(1+exp(x)),1)
    }
  ) +
  theme(
    legend.position = 'bottom'
  )


time_out_cor_g2_multi = logit_rates %>%
  ggplot(
    aes(
      x = time,
      y = outcome_norm
    )
  ) +
  geom_point(size = 2, color = 'firebrick') +
  geom_abline(
    data = filter(slope_table5, outcome == 'outcome_norm'),
    aes(
      intercept = intercept,
      slope = slope,
      color = num_predictors
    ),
    size = 1.5
  ) +
  geom_abline(
    data = NULL,
    slope = unadjusted_norm[1],
    intercept = unadjusted_norm[2],
    color = 'black',
    linetype = 'dashed',
    size = 1.5
  ) +
  theme_minimal() +
  labs(
    title = 'Normal Outcome',
    y = 'Mean',
    x = 'Time',
    color = 'Strength'
  ) +
  scale_colour_gradient(low = "thistle1", high = "darkslateblue", na.value = NA) +
  theme(
    legend.position = 'bottom'
  )

grid.arrange(
  time_out_cor_g1_multi, 
  time_out_cor_g2_multi, 
  nrow = 1,
  top = textGrob("20 models with multiple time & outcome correlated predictor")
)

```

Here we can see some very ugly lines of best fit. These look even worse than the single predictor models and I interpret this is there being a cumulative effect as we add more predictors. The fact that the lines don't fan out as uniformly is probably due to the fact that I sampled different incidences. This indicates that the incidence of the confounder variable also has an impact. I previously tried this experiment with some negatively correlated predictors as well and found that sometimes the effects pushed in opposite directions and the graph ended up not looking too bad.

Sometimes when I think about this, it makes intuitive sense. If the frequency or the mean of the confoudner increases with time and that variable is also predictive of the outcome, we would expect the predictions later in the time period to be greater than they would be if we used time alone as a predictor. Our model is predicting outcomes along multiple dimensions which are interrelated and when we look at only two axes, it hides these other dimensions.

Hopefully I will have some time to look at this a bit more later but for now my takeaways are:  
  
1. Predictors that are associated with only time or the outcome don't affect the outcome much  
2. Predictors that are associated with *both* time and the outcome really mess up graph  
3. In practice, you are likely to want to include variables that are correlated with both the outcome and primary exposure, otherwise they wouldn't be confounders! At the moment I don't have any remedy for this at the moment so I suspect that this method of plotting won't work in general.



